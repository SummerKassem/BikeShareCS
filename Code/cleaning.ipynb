{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil # to copy files across directories\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import calendar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is no CleanedData directory, then create one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a directory CleanedData if it does not exist \n",
    "def create_cleaning_directory(clean_dir):\n",
    "\n",
    "    if not os.path.exists(clean_dir):\n",
    "        os.makedirs(clean_dir)\n",
    "        print(f\"Directory '{clean_dir}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Directory '{clean_dir}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_dir = \"../CleanedData\"\n",
    "create_cleaning_directory(cleaning_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to have the data in an accessible form. This is done by reading the data into a dictionary of panada dataframes. Each element in the dictionary has a key (which is the name of the month the data was collected in) and a value (the csv entries in a panda dataframe) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dictionary containing the data from the .csv files\n",
    "def read_data(dir):\n",
    "    dfs = {}\n",
    "    files = os.listdir(dir)\n",
    "\n",
    "    # filter files by .csv bec some are google sheets\n",
    "    # extract the number of the month, then sort by month so that they are then stored in order of month\n",
    "    csv_files = sorted([int(file[4:6]) for file in files if file.endswith('.csv')])\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        # reconstruct the name of file bec we extracted the month number to sort them by month\n",
    "        reconstructed_name = '2023'+str(csv_file)+'-divvy-tripdata.csv' if csv_file >= 10 else '20230'+str(csv_file)+'-divvy-tripdata.csv'\n",
    "        f = os.path.join(dir, reconstructed_name) \n",
    "        \n",
    "        # get month name\n",
    "        month = calendar.month_name[csv_file]\n",
    "        \n",
    "        # read csv file into dataframe\n",
    "        dfs[month] = pd.read_csv(f)\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dir = \"../../BikeShareData/OriginalData\"\n",
    "data = read_data(original_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the number of data files (a check that all eleven months were read into the dictionary), and names of the months. We can also take a look at the first 5 entries for a couple of months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "dict_keys(['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CBCD0D7777F0E45F</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2023-02-14 11:59:42</td>\n",
       "      <td>2023-02-14 12:13:38</td>\n",
       "      <td>Southport Ave &amp; Clybourn Ave</td>\n",
       "      <td>TA1309000030</td>\n",
       "      <td>Clark St &amp; Schiller St</td>\n",
       "      <td>TA1309000024</td>\n",
       "      <td>41.920771</td>\n",
       "      <td>-87.663712</td>\n",
       "      <td>41.907993</td>\n",
       "      <td>-87.631501</td>\n",
       "      <td>casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F3EC5FCE5FF39DE9</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2023-02-15 13:53:48</td>\n",
       "      <td>2023-02-15 13:59:08</td>\n",
       "      <td>Clarendon Ave &amp; Gordon Ter</td>\n",
       "      <td>13379</td>\n",
       "      <td>Sheridan Rd &amp; Lawrence Ave</td>\n",
       "      <td>TA1309000041</td>\n",
       "      <td>41.957879</td>\n",
       "      <td>-87.649584</td>\n",
       "      <td>41.969517</td>\n",
       "      <td>-87.654691</td>\n",
       "      <td>casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E54C1F27FA9354FF</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2023-02-19 11:10:57</td>\n",
       "      <td>2023-02-19 11:35:01</td>\n",
       "      <td>Southport Ave &amp; Clybourn Ave</td>\n",
       "      <td>TA1309000030</td>\n",
       "      <td>Aberdeen St &amp; Monroe St</td>\n",
       "      <td>13156</td>\n",
       "      <td>41.920771</td>\n",
       "      <td>-87.663712</td>\n",
       "      <td>41.880419</td>\n",
       "      <td>-87.655519</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3D561E04F739CC45</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2023-02-26 16:12:05</td>\n",
       "      <td>2023-02-26 16:39:55</td>\n",
       "      <td>Southport Ave &amp; Clybourn Ave</td>\n",
       "      <td>TA1309000030</td>\n",
       "      <td>Franklin St &amp; Adams St (Temp)</td>\n",
       "      <td>TA1309000008</td>\n",
       "      <td>41.920873</td>\n",
       "      <td>-87.663733</td>\n",
       "      <td>41.879434</td>\n",
       "      <td>-87.635504</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0CB4B4D53B2DBE05</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2023-02-20 11:55:23</td>\n",
       "      <td>2023-02-20 12:05:48</td>\n",
       "      <td>Prairie Ave &amp; Garfield Blvd</td>\n",
       "      <td>TA1307000160</td>\n",
       "      <td>Cottage Grove Ave &amp; 63rd St</td>\n",
       "      <td>KA1503000054</td>\n",
       "      <td>41.794827</td>\n",
       "      <td>-87.618795</td>\n",
       "      <td>41.780531</td>\n",
       "      <td>-87.605970</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ride_id  rideable_type           started_at             ended_at  \\\n",
       "0  CBCD0D7777F0E45F   classic_bike  2023-02-14 11:59:42  2023-02-14 12:13:38   \n",
       "1  F3EC5FCE5FF39DE9  electric_bike  2023-02-15 13:53:48  2023-02-15 13:59:08   \n",
       "2  E54C1F27FA9354FF   classic_bike  2023-02-19 11:10:57  2023-02-19 11:35:01   \n",
       "3  3D561E04F739CC45  electric_bike  2023-02-26 16:12:05  2023-02-26 16:39:55   \n",
       "4  0CB4B4D53B2DBE05  electric_bike  2023-02-20 11:55:23  2023-02-20 12:05:48   \n",
       "\n",
       "             start_station_name start_station_id  \\\n",
       "0  Southport Ave & Clybourn Ave     TA1309000030   \n",
       "1    Clarendon Ave & Gordon Ter            13379   \n",
       "2  Southport Ave & Clybourn Ave     TA1309000030   \n",
       "3  Southport Ave & Clybourn Ave     TA1309000030   \n",
       "4   Prairie Ave & Garfield Blvd     TA1307000160   \n",
       "\n",
       "                end_station_name end_station_id  start_lat  start_lng  \\\n",
       "0         Clark St & Schiller St   TA1309000024  41.920771 -87.663712   \n",
       "1     Sheridan Rd & Lawrence Ave   TA1309000041  41.957879 -87.649584   \n",
       "2        Aberdeen St & Monroe St          13156  41.920771 -87.663712   \n",
       "3  Franklin St & Adams St (Temp)   TA1309000008  41.920873 -87.663733   \n",
       "4    Cottage Grove Ave & 63rd St   KA1503000054  41.794827 -87.618795   \n",
       "\n",
       "     end_lat    end_lng member_casual  \n",
       "0  41.907993 -87.631501        casual  \n",
       "1  41.969517 -87.654691        casual  \n",
       "2  41.880419 -87.655519        member  \n",
       "3  41.879434 -87.635504        member  \n",
       "4  41.780531 -87.605970        member  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "no_of_files = len(data)\n",
    "print(no_of_files)\n",
    "month_names = data.keys()\n",
    "print(month_names)\n",
    "#data[\"January\"].head()\n",
    "data[\"February\"].head()\n",
    "#data[\"April\"].head()\n",
    "#data[\"November\"].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The check_entries method will do the following:\n",
    " * check whether or not to remove duplicates\n",
    " * calculate number of entires in each file\n",
    " * calculate number of columns in each file\n",
    " * calculate the average and total number of entries across all files\n",
    " * write these outputs to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_entries(file_name, flag):\n",
    "    with open(file_name, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Month\",\"No Of Entries\",\"No Of Cols\"])\n",
    "\n",
    "        total = 0\n",
    "\n",
    "        for month, df in data.items():\n",
    "            if flag:\n",
    "                df.drop_duplicates()\n",
    "                \n",
    "            entries = len(df)\n",
    "            writer.writerow([month, entries,len(df.columns)])\n",
    "            total += len(df)\n",
    "            print(f\"For {month}:{df.shape}\")\n",
    "\n",
    "        average = int(total/11)\n",
    "        writer.writerow([\"Total:\", total])\n",
    "        writer.writerow([\"Average:\", average])\n",
    "        print(f\"Average number of entries per file: {average}\")\n",
    "        print(f\"Total number of entries across all files: {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For January:(190301, 13)\n",
      "For February:(190445, 13)\n",
      "For March:(258678, 13)\n",
      "For April:(426590, 13)\n",
      "For May:(604827, 13)\n",
      "For June:(719618, 13)\n",
      "For July:(767650, 13)\n",
      "For August:(771693, 13)\n",
      "For September:(666371, 13)\n",
      "For October:(537113, 13)\n",
      "For November:(362518, 13)\n",
      "Average number of entries per file: 499618\n",
      "Total number of entries across all files: 5495804\n"
     ]
    }
   ],
   "source": [
    "check_entries(\"Original_BikeRides.csv\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For January:(190301, 13)\n",
      "For February:(190445, 13)\n",
      "For March:(258678, 13)\n",
      "For April:(426590, 13)\n",
      "For May:(604827, 13)\n",
      "For June:(719618, 13)\n",
      "For July:(767650, 13)\n",
      "For August:(771693, 13)\n",
      "For September:(666371, 13)\n",
      "For October:(537113, 13)\n",
      "For November:(362518, 13)\n",
      "Average number of entries per file: 499618\n",
      "Total number of entries across all files: 5495804\n"
     ]
    }
   ],
   "source": [
    "check_entries(\"BikeRides_without_Duplicates.csv\", True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above print out, it is clear that all the files have the same number of columns. So that is a good preliminary check on the consistency of the data across the files. And we can see that in total we have 5.5 Million entries, with an average of 500000 entries per month. All calculated numbers before and after removing duplicates are identical, so the original dataset did not have any duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
