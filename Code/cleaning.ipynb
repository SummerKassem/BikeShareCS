{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil # to copy files across directories\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import calendar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is no CleanedData directory, then create one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a directory CleanedData if it does not exist \n",
    "def create_cleaning_directory(clean_dir):\n",
    "\n",
    "    if not os.path.exists(clean_dir):\n",
    "        os.makedirs(clean_dir)\n",
    "        print(f\"Directory '{clean_dir}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Directory '{clean_dir}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_dir = \"../CleanedData\"\n",
    "create_cleaning_directory(cleaning_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to have the data in an accessible form. This is done by reading the data into a dictionary of panada dataframes. Each element in the dictionary has a key (which is the name of the month the data was collected in) and a value (the csv entries in a panda dataframe) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dictionary containing the data from the .csv files\n",
    "def read_data(dir):\n",
    "    dfs = {}\n",
    "    files = os.listdir(dir)\n",
    "\n",
    "    # filter files by .csv bec some are google sheets\n",
    "    # extract the number of the month, then sort by month so that they are then stored in order of month\n",
    "    csv_files = sorted([int(file[4:6]) for file in files if file.endswith('.csv')])\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        # reconstruct the name of file bec we extracted the month number to sort them by month\n",
    "        reconstructed_name = '2023'+str(csv_file)+'-divvy-tripdata.csv' if csv_file >= 10 else '20230'+str(csv_file)+'-divvy-tripdata.csv'\n",
    "        f = os.path.join(dir, reconstructed_name) \n",
    "        \n",
    "        # get month name\n",
    "        month = calendar.month_name[csv_file]\n",
    "        \n",
    "        # read csv file into dataframe\n",
    "        dfs[month] = pd.read_csv(f)\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dir = \"../../BikeShareData/OriginalData\"\n",
    "data = read_data(original_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see:\n",
    "* the number of data files \n",
    "* the names of the months (a check that all eleven months were read into the dictionary)\n",
    "* the first and last 5 entries for a couple of months\n",
    "* the number of unique values in May\n",
    "* unique values for specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "dict_keys(['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November'])\n",
      "Column Name:          NUnique\n",
      "ride_id               604827\n",
      "rideable_type              3\n",
      "started_at            503683\n",
      "ended_at              505259\n",
      "start_station_name      1287\n",
      "start_station_id        1250\n",
      "end_station_name        1254\n",
      "end_station_id          1210\n",
      "start_lat             188591\n",
      "start_lng             185410\n",
      "end_lat                 4759\n",
      "end_lng                 4762\n",
      "member_casual              2\n",
      "dtype: int64\n",
      "['electric_bike' 'classic_bike' 'docked_bike']\n",
      "['member' 'casual']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "no_of_files = len(data)\n",
    "print(no_of_files)\n",
    "month_names = data.keys()\n",
    "print(month_names)\n",
    "#data[\"January\"]\n",
    "#data[\"February\"]\n",
    "#data[\"March\"]\n",
    "#data[\"April\"]\n",
    "#data[\"May\"].info()\n",
    "#data[\"June\"]\n",
    "#data[\"July\"]\n",
    "#data[\"August\"]\n",
    "#data[\"September\"]\n",
    "#data[\"October\"]\n",
    "#data[\"November\"]\n",
    "\n",
    "print(\"Column Name:          NUnique\")\n",
    "print(data['May'].nunique())\n",
    "print(data['May'].rideable_type.unique())\n",
    "print(data['May'].member_casual.unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count_entries method will do the following:\n",
    " * check whether or not to remove duplicates\n",
    " * calculate number of entires in each file\n",
    " * calculate number of columns in each file\n",
    " * calculate the average and total number of entries across all files\n",
    " * write these outputs to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entries(file_name, flag):\n",
    "    with open(file_name, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Month\",\"No Of Entries\",\"No Of Cols\"])\n",
    "\n",
    "        total = 0\n",
    "\n",
    "        for month, df in data.items():\n",
    "            if flag:\n",
    "                df = df.drop_duplicates()\n",
    "                \n",
    "            entries = len(df)\n",
    "            writer.writerow([month, entries,len(df.columns)])\n",
    "            total += len(df)\n",
    "            print(f\"For {month}:{df.shape}\")\n",
    "\n",
    "        average = int(total/11)\n",
    "        writer.writerow([\"Total:\", total])\n",
    "        writer.writerow([\"Average:\", average])\n",
    "        print(f\"Average number of entries per file: {average}\")\n",
    "        print(f\"Total number of entries across all files: {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For January:(190301, 13)\n",
      "For February:(190445, 13)\n",
      "For March:(258678, 13)\n",
      "For April:(426590, 13)\n",
      "For May:(604827, 13)\n",
      "For June:(719618, 13)\n",
      "For July:(767650, 13)\n",
      "For August:(771693, 13)\n",
      "For September:(666371, 13)\n",
      "For October:(537113, 13)\n",
      "For November:(362518, 13)\n",
      "Average number of entries per file: 499618\n",
      "Total number of entries across all files: 5495804\n"
     ]
    }
   ],
   "source": [
    "count_entries(\"Original_BikeRides.csv\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For January:(190301, 13)\n",
      "For February:(190445, 13)\n",
      "For March:(258678, 13)\n",
      "For April:(426590, 13)\n",
      "For May:(604827, 13)\n",
      "For June:(719618, 13)\n",
      "For July:(767650, 13)\n",
      "For August:(771693, 13)\n",
      "For September:(666371, 13)\n",
      "For October:(537113, 13)\n",
      "For November:(362518, 13)\n",
      "Average number of entries per file: 499618\n",
      "Total number of entries across all files: 5495804\n"
     ]
    }
   ],
   "source": [
    "count_entries(\"BikeRides_without_Duplicates.csv\", True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above print out, it is clear that all the files have the same number of columns. So that is a good preliminary check on the consistency of the data across the files. And we can see that in total we have 5.5 Million entries, with an average of 500000 entries per month. All calculated numbers before and after removing duplicates are identical, so the original dataset did not have any duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will look at the percentage of nulls across the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_NAN():\n",
    "    with open(\"NaN_Percentages.csv\", 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for month, df in data.items():\n",
    "            if month == 'January':\n",
    "                column_names = df.columns.insert(0, \"Month\")\n",
    "                #print(column_names)\n",
    "                writer.writerow(column_names)\n",
    "            \n",
    "            percentage = df.isna().sum()*100/len(df)\n",
    "            #y = percentage.apply(lambda x: str(int(x))+\"%\" if x > 1 else 0).values\n",
    "            y = percentage.apply(lambda x: str(int(x))+\"%\" if x > 1 else (\"< 1%\" if x > 0 else 0)).values\n",
    "            values = np.insert(y, 0, month)\n",
    "            print(values)\n",
    "            writer.writerow(values)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['January' 0 0 0 0 '14%' '14%' '14%' '14%' 0 0 '< 1%' '< 1%' 0]\n",
      "['February' 0 0 0 0 '13%' '13%' '14%' '14%' 0 0 '< 1%' '< 1%' 0]\n",
      "['March' 0 0 0 0 '13%' '13%' '14%' '14%' 0 0 '< 1%' '< 1%' 0]\n",
      "['April' 0 0 0 0 '14%' '14%' '16%' '16%' 0 0 '< 1%' '< 1%' 0]\n",
      "['May' 0 0 0 0 '14%' '14%' '15%' '15%' 0 0 '< 1%' '< 1%' 0]\n",
      "['June' 0 0 0 0 '16%' '16%' '17%' '17%' 0 0 '< 1%' '< 1%' 0]\n",
      "['July' 0 0 0 0 '16%' '16%' '16%' '16%' 0 0 '< 1%' '< 1%' 0]\n",
      "['August' 0 0 0 0 '15%' '15%' '16%' '16%' 0 0 '< 1%' '< 1%' 0]\n",
      "['September' 0 0 0 0 '15%' '15%' '16%' '16%' 0 0 '< 1%' '< 1%' 0]\n",
      "['October' 0 0 0 0 '15%' '15%' '16%' '16%' 0 0 '< 1%' '< 1%' 0]\n",
      "['November' 0 0 0 0 '15%' '15%' '15%' '15%' 0 0 '< 1%' '< 1%' 0]\n"
     ]
    }
   ],
   "source": [
    "check_NAN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the columns with NAN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drops_NANs(dataframe):\n",
    "    col_to_drop = ['start_station_name', 'start_station_id', 'end_station_name', 'end_station_id', 'start_lat', 'start_lng', 'end_lat', 'end_lng']\n",
    "\n",
    "    for _, df in dataframe.items():\n",
    "        #print(\"shape before\",df.shape)\n",
    "        df.drop(columns = col_to_drop, inplace=True) # drop column\n",
    "        #print(\"shape after\",df.shape)\n",
    "        #df.fillna('') # fill NaN\n",
    "        #df.dropnna(subset = \"Column Name\", inplace=True) # drop rows where column_name has NaN\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(190301, 5), (190445, 5), (258678, 5), (426590, 5), (604827, 5), (719618, 5), (767650, 5), (771693, 5), (666371, 5), (537113, 5), (362518, 5)]\n"
     ]
    }
   ],
   "source": [
    "#num = [df.shape for _,df in data.items()]\n",
    "#print(num)\n",
    "#data = read_data(original_dir)\n",
    "drops_NANs(data)\n",
    "num = [df.shape for _,df in data.items()]\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For January:(190301, 5)\n",
      "For February:(190445, 5)\n",
      "For March:(258678, 5)\n",
      "For April:(426590, 5)\n",
      "For May:(604827, 5)\n",
      "For June:(719618, 5)\n",
      "For July:(767650, 5)\n",
      "For August:(771693, 5)\n",
      "For September:(666371, 5)\n",
      "For October:(537113, 5)\n",
      "For November:(362518, 5)\n",
      "Average number of entries per file: 499618\n",
      "Total number of entries across all files: 5495804\n"
     ]
    }
   ],
   "source": [
    "count_entries(\"BikeRides_NaNsRemoved.csv\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
